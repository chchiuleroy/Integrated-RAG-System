# -*- coding: utf-8 -*-
"""
Integrated RAG System (Phase 1-3 Implementation)
æ•´åˆï¼šPDFè™•ç†(å«VLM) + Small-to-Big + Fusion Retrieval + Rerank + GraphRAG + CRAG + Query Transform
"""

import os
import json
import base64
import requests
import re
import datetime
import shutil
import jieba
import numpy as np
import pickle
from typing import List, Dict, Any, Tuple

# ç¬¬ä¸‰æ–¹å¥—ä»¶
import fitz  # pip install PyMuPDF
import pymupdf4llm # pip install pymupdf4llm
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from rank_bm25 import BM25Okapi
from sentence_transformers import CrossEncoder
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from neo4j import GraphDatabase
from duckduckgo_search import DDGS

# ==================== é…ç½®å€ (Configuration) ====================

class Config:
    # LLM & VLM API
    TEXT_API_URL = "http://10.2.6.150:3000/api/v1/prediction/98317c50-906a-4656-8b23-b94847d02a91"
    IMAGE_API_URL = "http://10.2.6.150:3000/api/v1/prediction/ad31badb-292b-4236-a148-1f1d123c7a3c"
    
    # Paths (Modified for Safety)
    BASE_DIR = os.path.join(os.getcwd(), "documents") # é è¨­æ”¹ç‚ºç•¶å‰ç›®éŒ„ä¸‹çš„ documents
    if not os.path.exists(BASE_DIR):
        try:
            os.makedirs(BASE_DIR)
        except:
            pass # æ¬Šé™ä¸è¶³æˆ–å…¶ä»–åŸå› å¿½ç•¥
            
    IMAGE_DIR = "pdf_images"
    DB_PATH = "rag_final_data.json" # ç”¨æ–¼å‚™ä»½çš„ä¸­é–“æª”
    
    # ChromaDB
    CHROMA_HOST = "localhost"
    CHROMA_PORT = 8000
    CHROMA_COLLECTION = "integrated_rag_system_v0"
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    
    # Neo4j
    NEO4J_URI = "bolt://localhost:7687"
    NEO4J_USER = "neo4j"
    NEO4J_PASSWORD = "password123"
    
    # Reranker
    RERANK_MODEL = "BAAI/bge-reranker-v2-m3" # è‹¥ç„¡ GPU å¯æ”¹ 'cross-encoder/ms-marco-MiniLM-L-6-v2'
    
    # Ontology (Lazy Load Logic)
    ONTOLOGY_PATH = "ontology.json"
    ENTITY_TYPES = ["Organization", "Person", "Location", "Event", "Concept", "Product"] # Default fallback
    RELATION_TYPES = ["RELATED", "PART_OF", "BELONGS_TO", "LOCATED_AT", "PARTICIPATED_IN"] # Default fallback

    @classmethod
    def load_ontology(cls):
        """å˜—è©¦è¼‰å…¥ ontology.jsonï¼Œè‹¥å¤±æ•—å‰‡ä½¿ç”¨é è¨­å€¼"""
        if os.path.exists(cls.ONTOLOGY_PATH):
            try:
                with open(cls.ONTOLOGY_PATH, "r", encoding="utf-8") as f:
                    _onto = json.load(f)
                    cls.ENTITY_TYPES = _onto.get("entity_types", cls.ENTITY_TYPES)
                    cls.RELATION_TYPES = _onto.get("relation_types", cls.RELATION_TYPES)
                    print(f"âœ… Loaded Ontology: {cls.ENTITY_TYPES}")
            except Exception as e:
                print(f"âš ï¸ Failed to load ontology.json: {e}. Using defaults.")
        else:
            print("â„¹ï¸ ontology.json not found. Using default entity types.")

# åŸ·è¡Œè¼‰å…¥å˜—è©¦
Config.load_ontology()


# ==================== å·¥å…·é¡åˆ¥ (Helpers) ====================

class LLMClient:
    """çµ±ä¸€è™•ç† LLM èˆ‡ VLM çš„å‘¼å«"""
    
    @staticmethod
    def query_text(prompt: str) -> str:
        try:
            resp = requests.post(Config.TEXT_API_URL, json={"question": prompt}, timeout=60)
            resp.raise_for_status()
            return resp.json().get('text', '').strip()
        except Exception as e:
            print(f"âš ï¸ [LLM Error] {e}")
            return ""

    @staticmethod
    def query_image(prompt: str, image_path: str) -> str:
        try:
            if not os.path.exists(image_path): return ""
            
            # Encode image
            mime_type = "image/png"
            if image_path.lower().endswith(".jpg"): mime_type = "image/jpeg"
            
            with open(image_path, "rb") as f:
                b64_str = base64.b64encode(f.read()).decode('utf-8')
                data_uri = f"data:{mime_type};base64,{b64_str}"
            
            payload = {
                "question": prompt,
                "uploads": [{
                    "data": data_uri,
                    "type": "file",
                    "name": os.path.basename(image_path),
                    "mime": mime_type
                }]
            }
            resp = requests.post(Config.IMAGE_API_URL, json=payload, timeout=90)
            return resp.json().get('text', '').strip()
        except Exception as e:
            print(f"âš ï¸ [VLM Error] {e}")
            return ""
    
    @staticmethod
    def clean_and_parse_json(text: str) -> dict:
        """è™•ç†åŒ…å« Markdown æ¨™ç±¤æˆ–å¤šé¤˜èªªæ˜çš„ JSON å­—ä¸²"""
        try:
            # ç§»é™¤ ```json ... ``` æ¨™ç±¤
            text = re.sub(r"```json|```", "", text).strip()
            # å°‹æ‰¾ç¬¬ä¸€å€‹ { æˆ– [ åˆ°æœ€å¾Œä¸€å€‹ } æˆ– ]
            match = re.search(r"(\{.*\}|\[.*\])", text, re.DOTALL)
            if match:
                return json.loads(match.group())
            return json.loads(text)
        except Exception as e:
            print(f"âš ï¸ JSON è§£æå¤±æ•—: {e}")
            return {}

# ==================== åœ–è­œç¶­è­· =================================== #

class GraphMaintenance:
    def __init__(self, driver):
        self.driver = driver

    def merge_similar_entities(self, batch_size=50):
        """
        é›¢ç·šæ‰¹æ¬¡è™•ç†:ä¸€æ¬¡è®€å–å¤§é‡å¯¦é«”,åˆ†çµ„é€äº¤ LLM åˆ¤æ–·ç›¸ä¼¼æ€§
        """
        print("--- å•Ÿå‹•é›¢ç·šå¯¦é«”æ¶ˆè§£ (Batch Entity Resolution) ---")
        
        with self.driver.session() as session:
            # æŒ‰é¡åˆ¥åˆ†çµ„æŠ“å–å¯¦é«”
            for ent_type in Config.ENTITY_TYPES:
                print(f"  > æ­£åœ¨è™•ç†é¡åˆ¥: {ent_type}")
                query = "MATCH (e:Entity {type: $etype}) RETURN e.name AS name"
                result = session.run(query, etype=ent_type)
                all_names = [r["name"] for r in result]
                
                if len(all_names) < 2: 
                    continue
                
                # åˆ†æ‰¹æ¬¡é€çµ¦ LLM
                for i in range(0, len(all_names), batch_size):
                    batch = all_names[i : i + batch_size]
                    
                    prompt = f"""
                            ä½ æ˜¯ä¸€å€‹å°ˆå®¶ç´šæ•¸æ“šæ¸…æ´—å¸«ã€‚è«‹åˆ†æä»¥ä¸‹é¡åˆ¥ç‚º '{ent_type}' çš„å¯¦é«”æ¸…å–®ã€‚
                            æ‰¾å‡ºã€ŒæŒ‡ä»£åŒä¸€å€‹å°è±¡ã€çš„å¯¦é«”(ä¾‹å¦‚:'TSMC' èˆ‡ 'å°ç©é›»')ã€‚
                            è‹¥æœ‰ç›¸ä¼¼è€…,è«‹é¸å‡ºä¸€å€‹æœ€åˆé©çš„åç¨±ä½œç‚ºä¸»å¯¦é«”ã€‚
                            è¼¸å‡ºæ ¼å¼ JSON: {{"merges": [ {{"primary": "ä¸»åç¨±", "aliases": ["åˆ¥å1", "åˆ¥å2"]}} ]}}
                            
                            å¾…è™•ç†æ¸…å–®: {batch}
                            """
                    
                    res_text = LLMClient.query_text(prompt)
                    res_data = LLMClient.clean_and_parse_json(res_text)
                    
                    for merge_task in res_data.get("merges", []):
                        primary = merge_task.get("primary")
                        aliases = merge_task.get("aliases", [])
                        if primary and aliases:
                            self._execute_merge_by_type(primary, aliases)

    def _execute_merge_by_type(self, primary_name: str, aliases: list):
        """
        æ–¹æ¡ˆ B-1: æŒ‰é—œä¿‚é¡å‹åˆ†åˆ¥è™•ç† (ä¿ç•™åŸå§‹é¡å‹)
        ç¼ºé»: éœ€è¦äº‹å…ˆçŸ¥é“æ‰€æœ‰é—œä¿‚é¡å‹
        """
        with self.driver.session() as session:
            try:
                # Step 1: ç¢ºä¿ä¸»ç¯€é»å­˜åœ¨
                session.run("MERGE (target:Entity {name: $primary})", 
                           primary=primary_name)
                
                # Step 2: éæ­·æ¯å€‹é—œä¿‚é¡å‹é€²è¡Œåˆä½µ
                for rel_type in Config.RELATION_TYPES:
                    # 2a. è½‰ç§»è©²é¡å‹çš„é€²å…¥é—œä¿‚
                    session.run(f"""
                        MATCH (target:Entity {{name: $primary}})
                        MATCH (alias:Entity) WHERE alias.name IN $aliases
                        MATCH (src)-[r:{rel_type}]->(alias)
                        MERGE (src)-[newR:{rel_type}]->(target)
                        SET newR += properties(r),
                            newR.migrated_at = datetime(),
                            newR.confidence = coalesce(r.confidence, 1) + coalesce(newR.confidence, 0)
                        DELETE r
                    """, primary=primary_name, aliases=aliases)
                    
                    # 2b. è½‰ç§»è©²é¡å‹çš„å‡ºå»é—œä¿‚
                    session.run(f"""
                        MATCH (target:Entity {{name: $primary}})
                        MATCH (alias:Entity) WHERE alias.name IN $aliases
                        MATCH (alias)-[r:{rel_type}]->(dst)
                        MERGE (target)-[newR:{rel_type}]->(dst)
                        SET newR += properties(r),
                            newR.migrated_at = datetime(),
                            newR.confidence = coalesce(r.confidence, 1) + coalesce(newR.confidence, 0)
                        DELETE r
                    """, primary=primary_name, aliases=aliases)
                
                # Step 3: è™•ç†æœªçŸ¥é¡å‹çš„é—œä¿‚ (çµ±ä¸€ç‚º RELATED)
                # 3a. é€²å…¥é—œä¿‚
                session.run("""
                    MATCH (target:Entity {name: $primary})
                    MATCH (alias:Entity) WHERE alias.name IN $aliases
                    MATCH (src)-[r]->(alias)
                    MERGE (src)-[newR:RELATED]->(target)
                    SET newR = properties(r),
                        newR.original_type = type(r),
                        newR.migrated_at = datetime()
                    DELETE r
                """, primary=primary_name, aliases=aliases)
                
                # 3b. å‡ºå»é—œä¿‚
                session.run("""
                    MATCH (target:Entity {name: $primary})
                    MATCH (alias:Entity) WHERE alias.name IN $aliases
                    MATCH (alias)-[r]->(dst)
                    MERGE (target)-[newR:RELATED]->(dst)
                    SET newR = properties(r),
                        newR.original_type = type(r),
                        newR.migrated_at = datetime()
                    DELETE r
                """, primary=primary_name, aliases=aliases)
                
                # Step 4: åˆä½µå±¬æ€§ä¸¦åˆªé™¤åˆ¥åç¯€é»
                result = session.run("""
                    MATCH (target:Entity {name: $primary})
                    MATCH (alias:Entity) WHERE alias.name IN $aliases
                    SET target.aliases = coalesce(target.aliases, []) + collect(alias.name),
                        target.merged_count = coalesce(target.merged_count, 0) + count(alias),
                        target.last_merge_time = datetime()
                    WITH target, collect(alias) AS aliases_to_delete
                    UNWIND aliases_to_delete AS alias
                    DETACH DELETE alias
                    RETURN target.name AS merged_entity, size(aliases_to_delete) AS count
                """, primary=primary_name, aliases=aliases)
                
                record = result.single()
                if record and record['count'] > 0:
                    print(f"   âœ… [Type-Based Merged] {aliases} -> {record['merged_entity']} (åˆä½µäº† {record['count']} å€‹ç¯€é»)")
                else:
                    print(f"   âš ï¸ [Merge Warning] æœªæ‰¾åˆ°åˆ¥åç¯€é»æˆ–å·²åˆä½µ: {aliases}")
                    
            except Exception as e:
                print(f"   âŒ [Merge Error] {e}")

    def _execute_merge_unified(self, primary_name: str, aliases: list):
        """
        æ–¹æ¡ˆ B-2: çµ±ä¸€é—œä¿‚é¡å‹ç‚º RELATED (æœ€ç°¡å–®ä½†æœƒä¸Ÿå¤±é¡å‹ä¿¡æ¯)
        å„ªé»: ä¸éœ€è¦é å®šç¾©é—œä¿‚é¡å‹,ç¨‹å¼ç¢¼ç°¡æ½”
        ç¼ºé»: åŸå§‹é—œä¿‚é¡å‹æœƒå­˜åœ¨ original_type å±¬æ€§ä¸­
        """
        with self.driver.session() as session:
            try:
                merge_query = """
                // 1. ç¢ºä¿ä¸»ç¯€é»å­˜åœ¨
                MERGE (target:Entity {name: $primary})
                
                // 2. è™•ç†æ‰€æœ‰åˆ¥åç¯€é»
                WITH target
                MATCH (alias:Entity) WHERE alias.name IN $aliases
                
                // 3. è½‰ç§»é€²å…¥é—œä¿‚
                WITH target, alias
                OPTIONAL MATCH (src)-[r_in]->(alias)
                WHERE r_in IS NOT NULL
                WITH target, alias, src, r_in, 
                     type(r_in) AS in_type, 
                     properties(r_in) AS in_props
                MERGE (src)-[new_in:RELATED]->(target)
                SET new_in = in_props,
                    new_in.original_type = in_type,
                    new_in.migrated_at = datetime()
                DELETE r_in
                
                // 4. è½‰ç§»å‡ºå»é—œä¿‚
                WITH target, alias
                OPTIONAL MATCH (alias)-[r_out]->(dst)
                WHERE r_out IS NOT NULL
                WITH target, alias, dst, r_out,
                     type(r_out) AS out_type,
                     properties(r_out) AS out_props
                MERGE (target)-[new_out:RELATED]->(dst)
                SET new_out = out_props,
                    new_out.original_type = out_type,
                    new_out.migrated_at = datetime()
                DELETE r_out
                
                // 5. åˆä½µå±¬æ€§ä¸¦åˆªé™¤
                WITH target, alias
                SET target.aliases = coalesce(target.aliases, []) + alias.name,
                    target.merged_count = coalesce(target.merged_count, 0) + 1
                DETACH DELETE alias
                
                RETURN target.name AS merged_entity, count(DISTINCT alias) AS merged_count
                """
                
                result = session.run(merge_query, primary=primary_name, aliases=aliases)
                record = result.single()
                if record and record['merged_count'] > 0:
                    print(f"   âœ… [Unified Merged] {aliases} -> {record['merged_entity']} (åˆä½µäº† {record['merged_count']} å€‹ç¯€é»)")
                else:
                    print(f"   âš ï¸ [Merge Warning] æœªæ‰¾åˆ°åˆ¥åç¯€é»: {aliases}")
                    
            except Exception as e:
                print(f"   âŒ [Merge Error] {e}")

    def verify_merge_results(self):
        """é©—è­‰åˆä½µçµæœçš„çµ±è¨ˆä¿¡æ¯"""
        with self.driver.session() as session:
            # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡å¯¦é«”
            duplicates = session.run("""
                MATCH (e:Entity)
                WITH e.type AS type, e.name AS name, count(*) AS cnt
                WHERE cnt > 1
                RETURN type, name, cnt
                ORDER BY cnt DESC
                LIMIT 10
            """)
            
            dup_list = list(duplicates)
            if dup_list:
                print("\nâš ï¸ ç™¼ç¾é‡è¤‡å¯¦é«”:")
                for rec in dup_list:
                    print(f"   - {rec['type']}: {rec['name']} (x{rec['cnt']})")
            else:
                print("\nâœ… æœªç™¼ç¾é‡è¤‡å¯¦é«”")
            
            # çµ±è¨ˆåˆä½µä¿¡æ¯
            stats = session.run("""
                MATCH (e:Entity)
                WHERE e.merged_count IS NOT NULL
                RETURN 
                    count(e) AS merged_entities,
                    sum(e.merged_count) AS total_merged,
                    avg(e.merged_count) AS avg_per_entity
            """).single()
            
            if stats and stats['merged_entities']:
                print("\nğŸ“Š åˆä½µçµ±è¨ˆ:")
                print(f"   - ä¸»å¯¦é«”æ•¸é‡: {stats['merged_entities']}")
                print(f"   - ç¸½åˆä½µç¯€é»: {stats['total_merged']}")
                print(f"   - å¹³å‡æ¯å€‹ä¸»å¯¦é«”åˆä½µ: {stats['avg_per_entity']:.2f} å€‹")

# ==================== å…¥åº«æµç¨‹ (Ingestion Pipeline) ==================== #

class RAGIngestor:
    def __init__(self):
        # åˆå§‹åŒ– Chroma
        self.emb_fn = SentenceTransformerEmbeddingFunction(model_name=Config.EMBEDDING_MODEL)
        self.chroma_client = chromadb.HttpClient(host=Config.CHROMA_HOST, port=Config.CHROMA_PORT)
        self.collection = self.chroma_client.get_or_create_collection(
            name=Config.CHROMA_COLLECTION, embedding_function=self.emb_fn
        )
        
        # åˆå§‹åŒ– Neo4j
        try:
            self.driver = GraphDatabase.driver(Config.NEO4J_URI, auth=(Config.NEO4J_USER, Config.NEO4J_PASSWORD))
            self.driver.verify_connectivity()
        except:
            print("âš ï¸ Neo4j é€£ç·šå¤±æ•—ï¼ŒGraphRAG åŠŸèƒ½å°‡åœç”¨")
            self.driver = None

    def process_pdf(self, pdf_path: str):
        """Phase 1: PDF çµæ§‹åŒ–è™•ç† (å« VLM)"""
        print(f"\n[1/4] Processing PDF: {os.path.basename(pdf_path)}")
        
        # æº–å‚™åœ–ç‰‡ç›®éŒ„
        base_name = os.path.splitext(os.path.basename(pdf_path))[0]
        img_output = os.path.join(os.path.dirname(pdf_path), base_name + "_images")
        if not os.path.exists(img_output): os.makedirs(img_output)
        
        # 1. è½‰ Markdown (å«è¡¨æ ¼é‚„åŸ)
        md_text = pymupdf4llm.to_markdown(pdf_path, write_images=True, image_path=img_output)
        
        # 2. åœ–ç‰‡èªæ„å¢å¼·
        lines = md_text.split('\n')
        new_lines = []
        for line in lines:
            new_lines.append(line)
            if line.strip().startswith("![](") and line.strip().endswith(")"):
                img_path = line.strip()[4:-1]
                if os.path.exists(img_path):
                    print(f"   -> Analyzing image: {os.path.basename(img_path)}")
                    caption = LLMClient.query_image("è©³ç´°æè¿°é€™å¼µåœ–ç‰‡çš„å…§å®¹ï¼ŒåŒ…å«åœ–è¡¨è¶¨å‹¢æˆ–è¡¨æ ¼æ•¸æ“šã€‚", img_path)
                    new_lines.append(f"\n> **[AI Image Analysis]**: {caption}\n")
        
        return "\n".join(new_lines)

    def _semantic_split(self, text: str, breakpoint_percentile=85) -> List[str]:
        """
        [Phase 1 å„ªåŒ–] èªæ„åˆ‡åˆ†å™¨ (Semantic Splitter)
        ä¸ä½¿ç”¨å›ºå®šå­—æ•¸ï¼Œè€Œæ˜¯æ ¹æ“šã€Œèªæ„ç›¸ä¼¼åº¦è®ŠåŒ–ã€ä¾†æ±ºå®šåˆ‡åˆ†é»ã€‚
        """
        # 1. ç°¡å–®åˆ†å¥ (è™•ç†ä¸­è‹±æ–‡å¥é»)
        single_sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+', text)
        single_sentences = [s for s in single_sentences if s.strip()]
        
        if len(single_sentences) < 2:
            return [text]

        # 2. è¨ˆç®—æ¯å€‹å¥å­çš„ Embedding
        try:
            embeddings = self.emb_fn(single_sentences)
        except Exception as e:
            print(f"      [Semantic Error] Embedding å¤±æ•—: {e}ï¼Œå›é€€è‡³ç´”æ–‡å­—")
            return [text]
        
        # 3. è¨ˆç®—ç›¸é„°å¥å­çš„ Cosine Distance
        distances = []
        for i in range(len(embeddings) - 1):
            v1 = np.array(embeddings[i])
            v2 = np.array(embeddings[i+1])
            sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
            distances.append(1 - sim)

        # 4. æ±ºå®šåˆ‡åˆ†é–¾å€¼ (ä½¿ç”¨ç™¾åˆ†ä½æ•¸)
        if not distances: return [text]
        threshold = np.percentile(distances, breakpoint_percentile)

        # 5. çµ„åˆ Chunks
        chunks = []
        current_chunk = ""
        for i, sentence in enumerate(single_sentences):
            current_chunk += sentence
            if i < len(distances) and distances[i] > threshold:
                if len(current_chunk) > 50: # é¿å…å¤ªç¢
                    chunks.append(current_chunk)
                    current_chunk = ""
        
        if current_chunk:
            chunks.append(current_chunk)
        return chunks

    def create_chunks(self, md_text: str, source_name: str) -> List[Dict]:
        """Phase 1: Small-to-Big åˆ‡åˆ† (å‡ç´šç‰ˆï¼šSemantic Child Chunking)"""
        print("[2/4] Chunking (Parent: Structure, Child: Semantic)...")
        
        # Parent Splitting (æŒ‰ç« ç¯€)
        headers = [("#", "H1"), ("##", "H2"), ("###", "H3")]
        parent_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers, strip_headers=False)
        parents = parent_splitter.split_text(md_text)
        
        chunks_data = []
        for i, p_doc in enumerate(parents):
            source_id = os.path.splitext(os.path.basename(source_name))[0]
            p_id = f"{source_id}_p{i}"
            
            # ä½¿ç”¨èªæ„åˆ‡åˆ†ä»£æ›¿ RecursiveCharacterTextSplitter
            children = self._semantic_split(p_doc.page_content, breakpoint_percentile=85)
            
            for j, c_text in enumerate(children):
                chunks_data.append({
                    "id": f"{p_id}_c{j}",
                    "parent_id": p_id,
                    "text_embedding": c_text,  # èªæ„èšåˆçš„å°å€å¡Š
                    "text_llm": p_doc.page_content, # å®Œæ•´ç« ç¯€
                    "metadata": {**p_doc.metadata, "source": source_name, "parent_id": p_id}
                })
        print(f"   -> Generated {len(chunks_data)} semantic chunks from {len(parents)} sections.")
        return chunks_data

    def augment_and_extract_graph(self, chunks: List[Dict]):
        print(f"[3/4] æå–åŸå§‹åœ–è­œæ•¸æ“š ({len(chunks)} chunks)...")
    
        for chunk in chunks:
            # 1. åƒ…åšå‡è¨­æ€§å•é¡Œç”Ÿæˆ
            prompt_aug = f"é‡å°ä»¥ä¸‹å…§å®¹ï¼Œç”Ÿæˆ3å€‹å•é¡Œï¼š\n{chunk['text_embedding'][:500]}"
            questions = LLMClient.query_text(prompt_aug)
            chunk['text_embedding'] += f"\n\n[Hypothetical Questions]:\n{questions}"
            chunk['metadata']['aug_questions'] = questions
    
            # 2. ä¿®æ”¹ï¼šæå–æ™‚ã€Œä¸ã€è¦æ±‚ LLM åšæ¶ˆè§£ï¼Œåªæå–åŸæ–‡ä¸­çš„å¯¦é«”å
            if self.driver:
                # é€™è£¡çš„ Prompt æ”¹ç‚ºåªæå–ï¼Œä¸æ­¸ä¸€åŒ–ï¼Œæ¸›å°‘ LLM æ¨ç†è² æ“”
                prompt_kg = f"è«‹æå–æ–‡æœ¬ä¸­çš„å¯¦é«”èˆ‡é—œä¿‚ã€‚ç›´æ¥æå–åŸæ–‡åç¨±å³å¯ï¼Œç„¡éœ€æ­¸ä¸€åŒ–ã€‚æ ¼å¼ JSON: {{'entities':[], 'relations':[]}}\næ–‡æœ¬ï¼š{chunk['text_embedding'][:1000]}"
                kg_json = LLMClient.query_text(prompt_kg)
                # å‘¼å«æˆ‘å€‘ä¹‹å‰å„ªåŒ–éçš„æ‰¹æ¬¡å¯«å…¥æ–¹æ³•
                self._save_to_neo4j(kg_json, chunk)
    
    def _entity_resolution_llm(self, raw_entities: List[Dict]) -> List[Dict]:
        """
        [å„ªåŒ–] å¯¦é«”å°é½Š (Entity Linking): 
        å°‡æå–å‡ºçš„åŸå§‹å¯¦é«”é€äº¤ LLM é€²è¡Œæ­¸ä¸€åŒ– (Normalization)
        """
        if not raw_entities: return []
        
        prompt = f"""
        ä½ æ˜¯ä¸€å€‹å°ˆå®¶ç´šçš„çŸ¥è­˜åœ–è­œå·¥ç¨‹å¸«ã€‚è«‹å°‡ä»¥ä¸‹å¯¦é«”æ¸…å–®é€²è¡Œæ­¸ä¸€åŒ–ï¼š
        1. çµ±ä¸€åç¨±ï¼šä¾‹å¦‚ 'å°ç©é›»', 'TSMC', 'å°ç£ç©é«”é›»è·¯' æ‡‰çµ±ä¸€ç‚º 'TSMC'ã€‚
        2. é¡åˆ¥å°é½Šï¼šå¿…é ˆå±¬æ–¼ {Config.ENTITY_TYPES} ä¹‹ä¸€ã€‚
        è¼¸å‡ºæ ¼å¼ç‚º JSON: {{"entities": [{{"original_name": "...", "resolved_name": "...", "type": "..."}}]}}
        
        å¾…è™•ç†æ¸…å–®ï¼š{raw_entities}
        """
        res = LLMClient.query_text(prompt)
        try:
            # ç°¡å–®æ­£å‰‡æå– JSON
            match = re.search(r"(\{.*\})", res, re.DOTALL)
            return json.loads(match.group()).get('entities', [])
        except:
            return raw_entities # å¤±æ•—å‰‡å›é€€

    def _save_to_neo4j(self, kg_text: str, chunk: Dict):
        try:
            # ä½¿ç”¨æ›´å¼·å¥çš„ JSON æå–
            data = LLMClient.clean_and_parse_json(kg_text)
            if not data: return
    
            current_time = datetime.datetime.now().strftime("%Y-%m-%d")
            
            # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
            ent_list = data.get('entities', [])
            rel_list = data.get('relations', [])
    
            with self.driver.session() as session:
                # 0. åœ¨è™•ç† entities ä¹‹å‰å…ˆç¢ºä¿ Chunk ç¯€é»å­˜åœ¨
                session.run("""
                    MERGE (c:Chunk {id: $cid})
                    SET c.source = $source, c.created_at = $time
                """, cid=chunk['id'], source=chunk['metadata'].get('source', 'unknown'), time=current_time)
                # 1. æ‰¹æ¬¡è™•ç†å¯¦é«”èˆ‡ MENTIONED_IN é—œä¿‚
                session.run("""
                    UNWIND $ents AS ent
                    MERGE (e:Entity {name: ent.name})
                    SET e.type = ent.type, e.last_updated = $time
                    WITH e
                    MATCH (c:Chunk {id: $cid})
                    MERGE (e)-[r:MENTIONED_IN]->(c)
                    SET r.detected_at = $time
                """, ents=ent_list, time=current_time, cid=chunk['id'])
    
                # 2. æ‰¹æ¬¡è™•ç†é—œä¿‚ (RELATED)
                session.run("""
                    UNWIND $rels AS rel
                    MATCH (a:Entity {name: rel.source})
                    MATCH (b:Entity {name: rel.target})
                    MERGE (a)-[r:RELATED {type: rel.type}]->(b)
                    ON CREATE SET r.confidence = 1, r.first_seen = $time
                    ON MATCH SET r.confidence = r.confidence + 1, r.last_seen = $time
                """, rels=rel_list, time=current_time)
                
        except Exception as e:
            print(f"âŒ Graph Batch Save Error: {e}")

    def save_to_db(self, chunks: List[Dict]):
        """
        [å„ªåŒ–ç‰ˆ] å°‡æ™‚é–“ç¶­åº¦å¯«å…¥ ChromaDB Metadata
        """
        print("[4/4] Saving to ChromaDB with temporal metadata...")
        ids = [c['id'] for c in chunks]
        docs = [c['text_embedding'] for c in chunks]
        
        # æº–å‚™ Metadataï¼ŒåŠ å…¥å‰µå»ºæ™‚é–“
        current_time_int = int(datetime.datetime.now().timestamp())
        metas = []
        for c in chunks:
            m = c['metadata'].copy()
            m["created_at"] = current_time_int  # ç”¨æ–¼æ•¸å€¼éæ¿¾
            m["date_string"] = datetime.datetime.now().strftime("%Y-%m-%d")
            metas.append(m)
        
        self.collection.upsert(ids=ids, documents=docs, metadatas=metas)
        
        # ä¿å­˜ä¸€ä»½å®Œæ•´çš„æ˜ å°„è¡¨ (Parent Content) åˆ°æœ¬åœ°ï¼Œä¾› Retrieval éšæ®µä½¿ç”¨
        # å¯¦å‹™ä¸Šé€™éƒ¨åˆ†æ‡‰è©²å­˜ Redis æˆ– SQLï¼Œé€™è£¡ç”¨ JSON æ¨¡æ“¬
        parent_map = {c['id']: c['text_llm'] for c in chunks}
        
        # å¦‚æœå­˜åœ¨èˆŠçš„æ˜ å°„è¡¨ï¼Œå‰‡åˆä½µ
        if os.path.exists("parent_map.json"):
            with open("parent_map.json", "r", encoding="utf-8") as f:
                old_map = json.load(f)
            parent_map.update(old_map)
            
        with open("parent_map.json", "w", encoding="utf-8") as f:
            json.dump(parent_map, f, ensure_ascii=False, indent=2)
            
        print("âœ… Ingestion Complete!")

# ==================== æª¢ç´¢æµç¨‹ (Retrieval Engine) ====================

class RAGRetriever:
    def __init__(self):
        # 1. ChromaDB
        self.emb_fn = SentenceTransformerEmbeddingFunction(model_name=Config.EMBEDDING_MODEL)
        self.client = chromadb.HttpClient(host=Config.CHROMA_HOST, port=Config.CHROMA_PORT)
        self.collection = self.client.get_collection(name=Config.CHROMA_COLLECTION, embedding_function=self.emb_fn)
        
        # 2. BM25 (éœ€å¾ Chroma æ’ˆå‡ºæ‰€æœ‰è³‡æ–™ä¾†å»ºç«‹ï¼Œæˆ–è®€å–æš«å­˜æª”)
        cache_path = "bm25_index.pkl"
        if os.path.exists(cache_path):
            with open(cache_path, "rb") as f:
                cache_data = pickle.load(f)
                self.bm25 = cache_data['bm25']
                self.bm25_docs = cache_data['docs']
                self.bm25_ids = cache_data['ids']
        else:
            print("Building BM25 index from scratch...")
            all_data = self.collection.get(include=["documents", "ids"])
            self.bm25_docs = all_data['documents']
            self.bm25_ids = all_data['ids']
            if not self.bm25_docs:
                print("âš ï¸ Warning: No documents found in ChromaDB. BM25 will be empty.")
                tokenized_corpus = []
            else:
                tokenized_corpus = [list(jieba.cut(doc)) for doc in self.bm25_docs]
            
            self.bm25 = BM25Okapi(tokenized_corpus) if tokenized_corpus else None
            
            with open(cache_path, "wb") as f:
                pickle.dump({
                    'bm25': self.bm25,
                    'docs': self.bm25_docs,
                    'ids': self.bm25_ids
                }, f)
            print("BM25 index built and cached.")
        
        # 3. Reranker
        print("Loading Reranker...")
        self.reranker = CrossEncoder(Config.RERANK_MODEL)
        
        # 4. Neo4j
        try:
            self.driver = GraphDatabase.driver(Config.NEO4J_URI, auth=(Config.NEO4J_USER, Config.NEO4J_PASSWORD))
        except: self.driver = None
        
        # 5. Parent Map (Small-to-Big) - [Modified for Safety]
        if os.path.exists("parent_map.json"):
            with open("parent_map.json", "r", encoding="utf-8") as f:
                self.parent_map = json.load(f)
        else:
            print("âš ï¸ parent_map.json not found. Small-to-Big retrieval may fail.")
            self.parent_map = {}
    

    def query_transform(self, query: str) -> List[str]:
        """Phase 2: Query Transformation (å¤šè·¯æŸ¥è©¢)"""
        print(f"   [Transform] Generating multi-queries for: {query}")
        prompt = f"è«‹é‡å°å•é¡Œ '{query}' ç”Ÿæˆ 3 å€‹ä¸åŒåˆ‡å…¥é»çš„æœå°‹é—œéµå­—æˆ–å•å¥ï¼Œä¸€è¡Œä¸€å€‹ã€‚"
        res = LLMClient.query_text(prompt)
        new_queries = [q.strip() for q in res.split('\n') if q.strip()]
        return [query] + new_queries[:3] # åŒ…å«åŸå•é¡Œ
    
    def extract_entities(self, query: str) -> List[str]:
        keywords = set(jieba.cut(query))
        entities = []
    
        if not self.driver:
            return []
    
        with self.driver.session() as session:
            for kw in keywords:
                if len(kw) < 2:
                    continue
                res = session.run(
                    "MATCH (e:Entity) WHERE e.name CONTAINS $kw RETURN e.name LIMIT 3",
                    kw=kw
                )
                entities.extend([r["e.name"] for r in res])
    
        return list(set(entities))

    def search_graph(self, query: str) -> List[str]:
        """Phase 3: GraphRAG æª¢ç´¢ (æ‰¾å‡ºé—œè¯å¯¦é«”)"""
        if not self.driver: return []
        
        # ç°¡å–®å¯¦ä½œï¼šç”¨é—œéµå­—å» Graph æ‰¾ Entityï¼Œå†æ‰¾ç›¸é€£çš„ Chunk
        # å¯¦å‹™ä¸Šéœ€å…ˆå° Query åš NER
        keywords = list(jieba.cut(query))
        found_ids = []
        with self.driver.session() as session:
            for kw in keywords:
                if len(kw) < 2: continue
                # æ‰¾æåŠè©²é—œéµå­—çš„ Chunk
                res = session.run("""
                    MATCH (e:Entity) WHERE e.name CONTAINS $kw
                    MATCH (e)-[:MENTIONED_IN]->(c:Chunk)
                    RETURN c.id LIMIT 5
                """, kw=kw)
                found_ids.extend([record["c.id"] for record in res])
        return list(set(found_ids))
    
    def graph_guided_chunk_ids(self, query: str, limit=50):
        if not self.driver:
            return [], []
    
        # 1ï¸âƒ£ æŠ½å– query entities
        entities = self.extract_entities(query)
        if not entities:
            return [], []
    
        # 2ï¸âƒ£ Neo4j æŸ¥ chunk
        cypher = """
        MATCH (e:Entity)-[:MENTIONED_IN]->(c:Chunk)
        WHERE e.name IN $entities
        RETURN DISTINCT c.id AS chunk_id
        LIMIT $limit
        """
    
        chunk_ids = []
    
        with self.driver.session() as session:
            result = session.run(cypher, entities=entities, limit=limit)
            for r in result:
                chunk_ids.append(r["chunk_id"])
    
        return chunk_ids, entities
    
    def get_reasoning_paths(self, entities: List[str], limit=5):
        """
        å›å‚³ï¼šEntity â†’ Relation â†’ Entity â†’ Chunk çš„æ¨ç†è·¯å¾‘
        """
        if not self.driver:
            return []
    
        cypher = """
        MATCH p=(e:Entity)-[r:RELATED*1..2]-(o:Entity)
        WHERE 
            e.name IN $entities
            AND all(x IN r WHERE x.confidence >= 2)
        MATCH (o)-[:MENTIONED_IN]->(c:Chunk)
        RETURN 
            [n IN nodes(p) | n.name] AS nodes,
            [rel IN relationships(p) | rel.type] AS relations,
            c.id AS chunk_id
        LIMIT $limit
        """
    
        with self.driver.session() as session:
            res = session.run(cypher, entities=entities, limit=limit)
            return [dict(r) for r in res]

    def retrieve(self, query: str, top_k=5) -> str:
        """
        0. Graph-guided -> 1. Query Transform -> 2. Multi-route Retrieval -> 3. RRF Fusion 
        -> 4. Temporal Rerank -> 5. Dynamic Top-K -> 6. CRAG
        """
        print(f"\n--- Processing Query: {query} ---")
        
        # 0. Graph-guided candidate pruning
        graph_ids, entities = self.graph_guided_chunk_ids(query)
        reasoning_paths = []
        if entities:
            # ä½¿ç”¨ query entityï¼ˆä½ å‰é¢å·²æŠ½éï¼‰
            reasoning_paths = self.get_reasoning_paths(entities)

        use_graph_filter = len(graph_ids) >= 5  # é¿å… Graph å¤ªå°èª¤å‚·
        
        # 1. Query Transformation (ç”¢ç”Ÿå¤šå€‹æœå°‹è®Šé«”)
        queries = self.query_transform(query) # å›å‚³ [åŸå•é¡Œ, è®Šé«”1, è®Šé«”2, è®Šé«”3]
        
        # 2. å¤šè·¯æª¢ç´¢èˆ‡ RRF èåˆ (Fusion)
        fused_scores = {}
        
        for q in queries:
            # A. Vector Search
            if use_graph_filter:
                v_res = self.collection.query(
                    query_texts=[q],
                    n_results=20,
                    ids=graph_ids
                )
            else:
                v_res = self.collection.query(
                    query_texts=[q],
                    n_results=20
                )
            # é˜²å‘†ï¼šé¿å… v_res ç‚ºç©º
            if v_res['ids']:
                for i, vid in enumerate(v_res['ids'][0]):
                    fused_scores[vid] = fused_scores.get(vid, 0) + 1/(60+i)
            
            # B. BM25 Search
            if self.bm25:
                b_docs = self.bm25.get_top_n(list(jieba.cut(q)), self.bm25_docs, n=20)
                for i, doc in enumerate(b_docs):
                    try:
                        idx = self.bm25_docs.index(doc)
                        bid = self.bm25_ids[idx]
                        scores = self.bm25.get_scores(list(jieba.cut(q)))  #å–®æ¬¡è¨ˆç®—ï¼Œä½†è‹¥loopå¤šï¼Œç´¯ç©O(N)
                        score = scores[idx]
                        if score <= 0:
                            continue
                        
                        if use_graph_filter and bid not in graph_ids:
                            continue
                        
                        fused_scores[bid] = fused_scores.get(bid, 0) + 1/(60+i)
                    except:
                        pass

        # 3. æå–å‰ 50 å€‹å€™é¸è€…é€²å…¥ Rerank
        candidates = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)[:50]
        candidate_ids = [c[0] for c in candidates]
        
        # å¦‚æœå®Œå…¨æ²’æœ‰å€™é¸æ–‡æª”
        if not candidate_ids:
            print("   [CRAG] ç„¡ç›¸é—œæ–‡æª”,å•Ÿå‹•å¤–éƒ¨æœç´¢...")
            try:
                with DDGS() as ddgs:
                    web_res = list(ddgs.text(query, max_results=3))
                web_context = "è£œå……å¤–éƒ¨è³‡è¨Š:\n" + "\n".join([r['body'] for r in web_res])
                return web_context, []
            except:
                return "ç„¡æ³•æ‰¾åˆ°ç›¸é—œè³‡æ–™,å¤–éƒ¨æœç´¢ä¹Ÿå¤±æ•—ã€‚", []

        # 4. ç²å– Metadata èˆ‡æº–å‚™ Rerank (Small-to-Big)
        res_meta = self.collection.get(ids=candidate_ids, include=['metadatas'])
        meta_dict = {cid: m for cid, m in zip(res_meta['ids'], res_meta['metadatas'])}
        
        pairs = []
        valid_ids = []
        seen_parents = set()
        for cid in candidate_ids:
            p_text = self.parent_map.get(cid) # é€é Child ID æ‰¾ Parent å®Œæ•´å…§å®¹
            if p_text and p_text not in seen_parents:
                pairs.append([query, p_text])
                valid_ids.append(cid)
                seen_parents.add(p_text)
        if not pairs:
            # ç›´æ¥å›é€€åˆ° fusion æ’åºçµæœ
            return "å…§éƒ¨æª¢ç´¢å¤±æ•—ï¼Œç„¡æ³•å–å¾—å®Œæ•´ä¸Šä¸‹æ–‡ã€‚", reasoning_paths

        # 5. åŸ·è¡Œ Cross-Encoder Rerank ä¸¦åŠ ä¸Šæ™‚é–“æ¬Šé‡
        raw_scores = self.reranker.predict(pairs)
        now_ts = int(datetime.datetime.now().timestamp())
        
        graph_chunk_id_set = set()

        if reasoning_paths:
            graph_chunk_id_set = {
                p["chunk_id"]
                for p in reasoning_paths
                if "chunk_id" in p
            }
        
        scored_results = []
        for i, score in enumerate(raw_scores):
            doc_id = valid_ids[i]
            metadata = meta_dict.get(doc_id, {})
            doc_ts = metadata.get("created_at", now_ts) # å¾ metadata æå–æ™‚é–“ç¶­åº¦
            
            # æ™‚é–“è¡°æ¸› (æ¯éš” 180 å¤©æ‰£ 0.1 åˆ†)
            time_penalty = ((now_ts - doc_ts) / 15552000) * 0.1
            adjusted_score = score - time_penalty
            
            # Graph reasoning boostï¼ˆæ¨ç†è­‰æ“šåŠ æ¬Šï¼‰
            if doc_id in graph_chunk_id_set:
                adjusted_score += 0.3
                
            scored_results.append({
                "score": adjusted_score,
                "text": pairs[i][1],
                "date": metadata.get("date_string", "Unknown")
            })
            
        scored_results.sort(key=lambda x: x['score'], reverse=True)

        # 6. å‹•æ…‹ Top-K ç¯©é¸
        best_score = scored_results[0]['score']
        dynamic_results = []
        for res in scored_results:
            # å‹•æ…‹éæ¿¾ï¼šåˆ†æ•¸å·®è·éå¤§æˆ–ä½æ–¼ä¿¡å¿ƒé–€æª»å‰‡æ¨æ£„
            if res['score'] > -1.5 and (best_score - res['score'] < 3.0):
                dynamic_results.append(res)
                if len(dynamic_results) >= top_k: break
        
        # 7. CRAG é–€æª»åˆ¤æ–· (æ±ºå®šæ˜¯å¦å•Ÿå‹• Web Search)
        if not dynamic_results or best_score < -2.5:
            print("   [CRAG] ä½ä¿¡å¿ƒåº¦,å•Ÿå‹•å¤–éƒ¨æœç´¢...")
            try:
                with DDGS() as ddgs:
                    web_res = list(ddgs.text(query, max_results=3))
                web_context = "è£œå……å¤–éƒ¨è³‡è¨Š:\n" + "\n".join([r['body'] for r in web_res])
                return web_context, reasoning_paths
            except:
                pass  # å¤–éƒ¨æœç´¢å¤±æ•—,ç¹¼çºŒä½¿ç”¨ç¾æœ‰çµæœ
        
        # 8. æœ€çµ‚è¼¸å‡º
        context_text = "\n\n".join([f"[Date: {r['date']}] {r['text']}" for r in dynamic_results])
        return context_text, reasoning_paths

# ==================== ä¸»åŸ·è¡Œå€ (Main) ====================

import argparse
import sys

# ==================== åŸ·è¡Œé‚è¼¯å°è£ (Workflow Functions) ====================

def run_full_ingestion():
    print("--- å•Ÿå‹•è‡ªå‹•åŒ–å…¥åº«æµç¨‹ (Ingestion) ---")
    ingestor = RAGIngestor()
    
    # æª¢æŸ¥ documents ç›®éŒ„æ˜¯å¦æœ‰ PDF
    pdf_files = [f for f in os.listdir(Config.BASE_DIR) if f.endswith(".pdf")]
    if not pdf_files:
        print(f"âš ï¸ Warning: No PDF files found in {Config.BASE_DIR}. Please add PDFs first.")
        return

    # ç¬¬ä¸€æ­¥ï¼šåªç®¡å…¥åº«ï¼ˆæ­¤æ™‚åœ–è­œä¸­å¯èƒ½æœ‰å¾ˆå¤šé‡è¤‡ç¯€é»ï¼Œå¦‚ 'å°ç©é›»' å’Œ 'TSMC'ï¼‰
    for f in pdf_files:
        path = os.path.join(Config.BASE_DIR, f)
        md = ingestor.process_pdf(path)
        chunks = ingestor.create_chunks(md, f)
        ingestor.augment_and_extract_graph(chunks) # é€™è£¡ç¾åœ¨å¾ˆå¿«ï¼Œå› ç‚ºä¸æ¶ˆè§£
        ingestor.save_to_db(chunks)
            
    # ç¬¬äºŒæ­¥ï¼šé›¢ç·šçµ±ä¸€æ¸…ç† (é€™å°±æ˜¯ä½ è¦æ±‚çš„æ”¹å‹•)
    if ingestor.driver:
        maintenance = GraphMaintenance(ingestor.driver)
        # æ­¤è™•åŸ·è¡Œå„ªåŒ–å¾Œçš„æ‰¹æ¬¡æ¶ˆè§£ï¼Œæ¥µå¤§ç¯€çœ Tokenï¼Œå› ç‚ºç›¸åŒå¯¦é«”åªæœƒè¢«åˆ¤æ–·ä¸€æ¬¡
        maintenance.merge_similar_entities(batch_size=40) 
        
    print("--- æ‰€æœ‰æµç¨‹å·²å®Œæˆ ---")

def run_qa_flow(retriever, query):
    """å°è£æª¢ç´¢èˆ‡å›ç­”çš„æµç¨‹é‚è¼¯"""
    context, reasoning_paths = retriever.retrieve(query)
    print("\n=== Context for LLM ===")
    print(context)
    print("=======================")
    
    # ç”Ÿæˆå›ç­”
    prompt = f"""
                åŸºæ–¼ä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œã€‚
                
                ã€æ¨ç†è·¯å¾‘ã€‘
                {json.dumps(reasoning_paths, ensure_ascii=False, indent=2)}
                
                ã€åƒè€ƒæ–‡ä»¶ã€‘
                {context}
                
                å•é¡Œï¼š{query}
                """
    ans = LLMClient.query_text(prompt)
    print(f"\nAI Answer:\n{ans}")
    return ans

# ==================== ä¸»åŸ·è¡Œå€ (Main) ====================

def main():
    parser = argparse.ArgumentParser(description="Integrated RAG System")
    parser.add_argument("--ingest", action="store_true", help="åŸ·è¡Œ PDF å…¥åº«æµç¨‹")
    parser.add_argument("--search", type=str, help="ç›´æ¥åŸ·è¡Œç‰¹å®šå•é¡Œæœå°‹")
    parser.add_argument("--cleanup", action="store_true", help="å–®ç¨åŸ·è¡Œåœ–è­œå»é‡åˆä½µ")
    args = parser.parse_args()

    # 1. è™•ç†å–®ç¨çš„åœ–è­œæ¸…ç†ä»»å‹™
    if args.cleanup:
        from neo4j import GraphDatabase
        driver = GraphDatabase.driver(Config.NEO4J_URI, auth=(Config.NEO4J_USER, Config.NEO4J_PASSWORD))
        maintenance = GraphMaintenance(driver)
        maintenance.merge_similar_entities()
        if not args.ingest and not args.search: return

    # 2. è™•ç†å…¥åº«ä»»å‹™ (CLI æ¨¡å¼)
    if args.ingest:
        run_full_ingestion()

    # 3. è™•ç†æœå°‹ä»»å‹™ (CLI æ¨¡å¼)
    if args.search:
        print(f"--- åŸ·è¡Œå–®æ¬¡æœå°‹: '{args.search}' ---")
        retriever = RAGRetriever()
        run_qa_flow(retriever, args.search)
        return

    # 4. äº’å‹•æ¨¡å¼ (ç•¶æ²’æœ‰æä¾›ä»»ä½•åƒæ•¸æ™‚è§¸ç™¼)
    if not args.ingest and not args.search:
        print("\n=== RAG ç³»çµ±äº’å‹•çµ‚ç«¯ ===")
        print(f"æª”æ¡ˆç›®éŒ„ (Base Dir): {Config.BASE_DIR}")
        print("1: åŒ¯å…¥ PDF (Ingest)")
        print("2: åŸ·è¡Œæª¢ç´¢ (Search)")
        print("3: åŸ·è¡Œåœ–è­œç¶­è­· (Cleanup)")
        mode = input("è«‹é¸æ“‡æ¨¡å¼: ")
        
        if mode == "1":
            run_full_ingestion()
        elif mode == "2":
            retriever = RAGRetriever()
            while True:
                q = input("\nè«‹è¼¸å…¥æ‚¨çš„å•é¡Œ (è¼¸å…¥ q é›¢é–‹): ")
                if q.lower() == 'q': break
                run_qa_flow(retriever, q)
        elif mode == "3":
            # è¤‡ç”¨ Cleanup é‚è¼¯
            from neo4j import GraphDatabase
            driver = GraphDatabase.driver(Config.NEO4J_URI, auth=(Config.NEO4J_USER, Config.NEO4J_PASSWORD))
            maintenance = GraphMaintenance(driver)
            maintenance.merge_similar_entities()

if __name__ == "__main__":
    main()